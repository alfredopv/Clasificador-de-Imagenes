{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDENTIFICADOR DE IMÁGENES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORIO = \"./comida\"\n",
    "CATEGORIAS = [\"alitas\", \"arroz\", \"bagel\", \"brownie\", \"croissant\", \"donas\", \"ensalada\", \"filete_de_res\", \n",
    "              \"hamburguesa\", \"hot_cakes\", \"hotdog\",\n",
    "             \"huevo\", \"lasagna\", \"muffin\", \"nachos\", \"nuggets_de_pollo\", \"paella\", \"palomitas\",\n",
    "             \"pan_tostado\", \"papas_a_la_francesa\", \"pastel\", \"pescado_frito\", \"pie_de_manzana\",\n",
    "             \"pizza\", \"salmon\", \"sandwich\", \"spaghetti\", \"sushi\", \"tacos\",\n",
    "             \"waffle\"]\n",
    "salida = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [00:02<00:00, 61.85it/s]\n",
      "100%|██████████| 114/114 [00:01<00:00, 88.65it/s]\n",
      "100%|██████████| 105/105 [00:01<00:00, 89.75it/s]\n",
      "100%|██████████| 105/105 [00:01<00:00, 53.04it/s]\n",
      "100%|██████████| 114/114 [00:00<00:00, 134.54it/s]\n",
      "100%|██████████| 103/103 [00:01<00:00, 80.43it/s]\n",
      "100%|██████████| 188/188 [00:02<00:00, 76.31it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 129.75it/s]\n",
      "100%|██████████| 240/240 [00:02<00:00, 85.36it/s]\n",
      "100%|██████████| 110/110 [00:01<00:00, 83.61it/s]\n",
      "100%|██████████| 109/109 [00:01<00:00, 104.93it/s]\n",
      "100%|██████████| 180/180 [00:02<00:00, 68.58it/s]\n",
      "100%|██████████| 108/108 [00:01<00:00, 77.66it/s]\n",
      "100%|██████████| 112/112 [00:01<00:00, 77.91it/s]\n",
      "100%|██████████| 101/101 [00:01<00:00, 52.19it/s]\n",
      "100%|██████████| 103/103 [00:01<00:00, 77.69it/s]\n",
      "100%|██████████| 105/105 [00:01<00:00, 73.34it/s]\n",
      "100%|██████████| 116/116 [00:01<00:00, 79.05it/s]\n",
      "100%|██████████| 216/216 [00:02<00:00, 101.20it/s]\n",
      "100%|██████████| 75/75 [00:00<00:00, 145.64it/s]\n",
      "100%|██████████| 212/212 [00:02<00:00, 82.06it/s]\n",
      "100%|██████████| 116/116 [00:01<00:00, 95.93it/s]\n",
      "100%|██████████| 101/101 [00:01<00:00, 86.38it/s]\n",
      "100%|██████████| 125/125 [00:01<00:00, 105.87it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 123.47it/s]\n",
      "100%|██████████| 163/163 [00:01<00:00, 117.70it/s]\n",
      "100%|██████████| 151/151 [00:01<00:00, 80.23it/s]\n",
      "100%|██████████| 136/136 [00:01<00:00, 121.08it/s]\n",
      "100%|██████████| 106/106 [00:01<00:00, 86.75it/s]\n",
      "100%|██████████| 102/102 [00:01<00:00, 85.66it/s]\n"
     ]
    }
   ],
   "source": [
    "for categoria in CATEGORIAS:  #categorías\n",
    "    path = os.path.join(DIRECTORIO,categoria)  # acceso a cada directorio de las categorías\n",
    "    numero_categoria = CATEGORIAS.index(categoria)\n",
    "    for img in tqdm(os.listdir(path)):  # Iterar cada imagen de perros y gatos\n",
    "        try:\n",
    "            img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # Convertir en un arreglo \n",
    "            nueva_img = cv2.resize(img_array, (70,70))\n",
    "            salida.append([nueva_img, numero_categoria])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "for matriz, etiqueta in salida:\n",
    "    X.append(matriz)\n",
    "    y.append(etiqueta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X).reshape(-1,70,70,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "pSalida = open(\"X.pickle\",\"wb\")\n",
    "pickle.dump(X, pSalida)\n",
    "pSalida.close()\n",
    "\n",
    "pSalida = open(\"y.pickle\",\"wb\")\n",
    "pickle.dump(y, pSalida)\n",
    "pSalida.close()\n",
    "\n",
    "pEntrada = open(\"X.pickle\",\"rb\")\n",
    "X = pickle.load(pEntrada)\n",
    "\n",
    "pEntrada = open(\"y.pickle\",\"rb\")\n",
    "y = pickle.load(pEntrada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = Sequential()\n",
    "modelo.add(Conv2D(64, (3,3), input_shape=X.shape[1:]))\n",
    "modelo.add(Activation(\"relu\"))\n",
    "modelo.add(MaxPooling2D(pool_size=(2,2)))\n",
    "modelo.add(Conv2D(64, (3,3)))\n",
    "modelo.add(Activation(\"relu\"))\n",
    "modelo.add(MaxPooling2D(pool_size=(2,2)))\n",
    "modelo.add(Conv2D(64, (3,3)))\n",
    "modelo.add(Activation(\"relu\"))\n",
    "modelo.add(MaxPooling2D(pool_size=(2,2)))\n",
    "modelo.add(Flatten())\n",
    "modelo.add(Dense(64))\n",
    "modelo.add(Dense(64))\n",
    "modelo.add(Dense(30))\n",
    "modelo.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(optimizer=\"adam\", loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3848\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_cate = to_categorical(y)\n",
    "print(len(y_cate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2693 samples, validate on 1155 samples\n",
      "Epoch 1/20\n",
      "2693/2693 [==============================] - 24s 9ms/step - loss: 3.3246 - acc: 0.0680 - val_loss: 3.2154 - val_acc: 0.0978\n",
      "Epoch 2/20\n",
      "2693/2693 [==============================] - 22s 8ms/step - loss: 3.0495 - acc: 0.1348 - val_loss: 3.1076 - val_acc: 0.1472\n",
      "Epoch 3/20\n",
      "2693/2693 [==============================] - 22s 8ms/step - loss: 2.7931 - acc: 0.2083 - val_loss: 2.7984 - val_acc: 0.1939\n",
      "Epoch 4/20\n",
      "2693/2693 [==============================] - 22s 8ms/step - loss: 2.5609 - acc: 0.2670 - val_loss: 2.7451 - val_acc: 0.2329\n",
      "Epoch 5/20\n",
      "2693/2693 [==============================] - 22s 8ms/step - loss: 2.3371 - acc: 0.3227 - val_loss: 2.9364 - val_acc: 0.1974\n",
      "Epoch 6/20\n",
      "2693/2693 [==============================] - 22s 8ms/step - loss: 2.1464 - acc: 0.3747 - val_loss: 2.9179 - val_acc: 0.2147\n",
      "Epoch 7/20\n",
      "2693/2693 [==============================] - 22s 8ms/step - loss: 1.9017 - acc: 0.4404 - val_loss: 2.9236 - val_acc: 0.2485\n",
      "Epoch 8/20\n",
      "2693/2693 [==============================] - 22s 8ms/step - loss: 1.6522 - acc: 0.5098 - val_loss: 3.0631 - val_acc: 0.2468\n",
      "Epoch 9/20\n",
      "2693/2693 [==============================] - 22s 8ms/step - loss: 1.3694 - acc: 0.5834 - val_loss: 3.3752 - val_acc: 0.2502\n",
      "Epoch 10/20\n",
      "2693/2693 [==============================] - 22s 8ms/step - loss: 1.1430 - acc: 0.6461 - val_loss: 3.6473 - val_acc: 0.2416\n",
      "Epoch 11/20\n",
      "2693/2693 [==============================] - 22s 8ms/step - loss: 0.8562 - acc: 0.7390 - val_loss: 3.9827 - val_acc: 0.2433\n",
      "Epoch 12/20\n",
      "2693/2693 [==============================] - 25s 9ms/step - loss: 0.6918 - acc: 0.7906 - val_loss: 4.5330 - val_acc: 0.2355\n",
      "Epoch 13/20\n",
      "2693/2693 [==============================] - 22s 8ms/step - loss: 0.5426 - acc: 0.8344 - val_loss: 5.0453 - val_acc: 0.2416\n",
      "Epoch 14/20\n",
      "2693/2693 [==============================] - 22s 8ms/step - loss: 0.3729 - acc: 0.8908 - val_loss: 5.5796 - val_acc: 0.2268\n",
      "Epoch 15/20\n",
      "2693/2693 [==============================] - 22s 8ms/step - loss: 0.2917 - acc: 0.9150 - val_loss: 5.8372 - val_acc: 0.2372\n",
      "Epoch 16/20\n",
      "2693/2693 [==============================] - 22s 8ms/step - loss: 0.2139 - acc: 0.9439 - val_loss: 6.4771 - val_acc: 0.2303\n",
      "Epoch 17/20\n",
      "2693/2693 [==============================] - 22s 8ms/step - loss: 0.1875 - acc: 0.9495 - val_loss: 6.4966 - val_acc: 0.2554\n",
      "Epoch 18/20\n",
      "2693/2693 [==============================] - 22s 8ms/step - loss: 0.1392 - acc: 0.9681 - val_loss: 6.9058 - val_acc: 0.2320\n",
      "Epoch 19/20\n",
      "2693/2693 [==============================] - 22s 8ms/step - loss: 0.1396 - acc: 0.9644 - val_loss: 7.4007 - val_acc: 0.2190\n",
      "Epoch 20/20\n",
      "2693/2693 [==============================] - 22s 8ms/step - loss: 0.2959 - acc: 0.9124 - val_loss: 6.9892 - val_acc: 0.2251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc2285ddf28>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.fit(X, y_cate, batch_size=32, epochs=20, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carga(rutaImg):\n",
    "    imgSize = 70\n",
    "    imgArray = cv2.imread(rutaImg, cv2.IMREAD_GRAYSCALE)\n",
    "    nImgArray = cv2.resize(imgArray, (imgSize, imgSize))\n",
    "    return nImgArray.reshape(-1, imgSize, imgSize, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comida? /home/alfredo/Documentos/UltimoSemestre/SistemasInteligentes/Clasificador-de-Imagenes/cuernito.jpg\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]]\n",
      "alitas\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "nombre = input(\"comida? \")\n",
    "p = modelo.predict([carga(nombre)])\n",
    "print(p)\n",
    "print(CATEGORIAS[int(p[0][0])])\n",
    "print(int(p[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Se programó un archivo para identificar la categoría o el tipo de alimento de una imágen. Se tienen aproximadamente 32 categorías diferentes de alimentos según algunos de los más conocidos de la alimentación mexicana. Para lograrlo primero se tuvo que limpiar el dataset eliminando categorías que no eran muy populares, remplazando imágenes que no fueran muy claras por otras y transformándolas en el código a blanco y negro y a arreglos para poder manejar la información. Se programó una red convolutiva con diferente número de neuronas, épocas y de capas para lograr un buen análisis obteniendo aproximadamente un 96% de efectividad. Sin embargo esto no fue suficiente y la predicción de este modelo sigue siendo muy pobre apenas detectando como 1 de 20 fotos que se le dan a analizar. Durante los próximos días, se probará utlizando un modelo ya entrenando y esperemos unos mejores resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
